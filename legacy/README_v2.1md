# Sovereign AI Factory (v2.1)

**Secured Enterprise-Grade Inference Infrastructure on Kubernetes**

This repository documents the evolution from a monolithic Docker Compose stack (v1.0) to a distributed **Kubernetes Architecture (v2.0)**, and now the integration of an adversarial **Red Team Layer (v2.1)**.

The system transforms a heterogeneous consumer GPU cluster into a tiered, high-availability inference engine using **NVIDIA Triton Inference Server**, guarded by a dedicated vulnerability scanning environment.

---

## 1. Executive Summary
**Status:** `PRE-ALPHA` // `RED TEAMING PHASE`

This project establishes a **Sovereign AI capability** independent of major cloud providers. The architecture consists of two distinct zones:
* **游댯 Blue Team (The Rig):** A 6-GPU local cluster for sensitive data inference.
* **游댮 Red Team (The Kill Box):** A network-segmented "Headless" VM for adversarial testing, prompt injection, and vulnerability scanning.

---

## 2. 游댯 Blue Team Architecture: The Tiered Approach
Unlike v1.0 which forced all cards to act as one giant GPU, the system segments the hardware into logical "Tiers" based on VRAM and Compute capability to eliminate bottlenecks.

### Hardware Inventory (Verified)
| Tier | Device ID | GPU Model | VRAM | Role |
| :--- | :--- | :--- | :--- | :--- |
| **Tier 1** | `GPU-0`, `GPU-5` | **RTX 3080** | 10GB (x2) | **Primary Inference Engine** (Mistral-Nemo). High-bandwidth cards paired for Tensor Parallelism. |
| **Tier 2** | `GPU-2` | **RTX 3060** | 12GB | **Context Worker**. Slower compute, but higher VRAM buffer. Reserved for future Agent/Coder models. |
| **Tier 3** | `GPU-1`, `GPU-3` | **RTX 3060 Ti** | 8GB (x2) | **Utility Pool**. Fast GDDR6 memory but small capacity. Reserved for Embeddings/Vectorization. |
| **Tier 3** | `GPU-4` | **RTX 3070** | 8GB | **Utility Pool**. Additional float throughput for batch processing. |

---

## 3. 游댮 Red Team Architecture: The Kill Box
A dedicated, isolated environment designed to attack the Blue Team infrastructure to validate security and robustness.

* **OS:** Ubuntu 22.04 LTS ("Headless")
* **Network:** `10.66.66.100` (Static/Hardened)
* **Target Endpoint:** `http://10.10.10.2:30400/v1` (LiteLLM Gateway)

### Arsenal & Tooling
| Tool | Purpose | Status |
| :--- | :--- | :--- |
| **Garak** | **The Machine Gun.** Automated LLM vulnerability scanner (Hallucination, injection probes). | 游릭 INSTALLED |
| **PyRIT** | **The Sniper.** Microsoft's agent-based adversarial attack framework (Red Bot vs Blue Bot). | 游릭 INSTALLED |
| **Promptfoo** | **The Unit Test.** Deterministic regression testing for system prompts. | 游릭 INSTALLED |
| **Caido** | **The X-Ray.** Lightweight HTTP/JSON interceptor for Man-in-the-Middle analysis. | 游릭 LISTENING |

---

## 4. Technology Stack (v2.1)

### Infrastructure
* **Orchestrator:** Kubernetes v1.31 (Bare Metal)
* **Inference Engine:** NVIDIA Triton Server 24.12 (Dec 2024 release)
* **Backend:** vLLM (PagedAttention capable)
* **API Gateway:** LiteLLM (Standardizes OpenAI format)
* **Storage:** Local HostPath NVMe (`/mnt/sovereign-storage`)

### Network Security
* **Segmentation:** Red Team VM is network-segmented via VLAN/Subnet but allowed strict access to the LiteLLM Gateway port.
* **Authentication:** API Key enforcement via LiteLLM (`sk-...`).
* **Monitoring:** Live request logging via `kubectl logs` to detect active probing.

---

## 5. Current Operational Status
| Component | Status | Notes |
| :--- | :--- | :--- |
| **VM Networking** | 游릭 **ONLINE** | Static IP `10.66.66.100` fixed via Netplan. |
| **Gateway (LiteLLM)** | 游릭 **ONLINE** | Routing traffic successfully. |
| **Inference (Triton)** | 游댮 **OFFLINE** | Debugging `Connect call failed`. Pending TensorRT optimization. |
| **Red Team Tools** | 游릭 **READY** | All Python dependencies and binaries installed. |

---

## 6. Operational Playbook

### A. Model Management
We bypass ephemeral container storage by mounting a dedicated host drive.
**Path:** `/mnt/sovereign-storage/models/mistral-nemo/1/`

### B. Deployment Manifests
The cluster uses a custom `RuntimeClass` to expose the NVIDIA drivers to K8s pods.

**Current Deployment:** `triton-stack.yaml`
* **Image:** `nvcr.io/nvidia/tritonserver:24.12-vllm-python-py3`
* **Resources:** Pinned to **Tier 1** via Triton Config.
* **Memory:** 32Gi Shared Memory (SHM) reserved for 32k Context Window.

### C. Engine Tuning
The vLLM engine is manually tuned to fit the 20GB VRAM envelope of Tier 1 while maximizing context.
* **Tensor Parallelism:** 2 (Spanning GPU-0 and GPU-5)
* **Precision:** BF16 (SafeTensors)

---
游늯 License MIT License.
